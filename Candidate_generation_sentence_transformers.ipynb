{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Candidate_generation_sentence-transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa3HgcUzM3lLTrCu+uz5l4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anil1055/Cantemist_Entity_Linking_IzunaCode/blob/main/Candidate_generation_sentence_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATn5SdXc6nnx"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install -U fuzzywuzzy\n",
        "!pip install textdistance\n",
        "!pip install obonet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/dataset/dev1.zip -d /content/dataset/dev1/\n",
        "!unzip /content/dataset/dev2.zip -d /content/dataset/dev2/\n",
        "!unzip /content/dataset/test.zip -d /content/dataset/test/\n",
        "!unzip /content/dataset/train.zip -d /content/dataset/train/"
      ],
      "metadata": {
        "id": "0XKfYT9HFVkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from annotations import parse_ner_output\n",
        "from candidates import write_candidates, generate_candidates_for_entity\n",
        "from cieo3 import load_cieo3\n",
        "from es_decs import load_es_decs\n",
        "from icd10cm import load_spanish_icd10cm\n",
        "from information_content import generate_ic_file\n",
        "#from strings import entity_string\n",
        "\n",
        "sys.path.append(\"./\")\n",
        "\n",
        "entity_string = \"ENTITY\\ttext:{0}\\tnormalName:{1}\\tpredictedType:{2}\\tq:true\"\n",
        "entity_string += \"\\tqid:Q{3}\\tdocId:{4}\\torigText:{0}\\turl:{5}\\n\"\n",
        "\n",
        "\n",
        "def build_entity_candidate_dict(annotations, model, min_match_score, ontology_graph, ontology_graph_2, ontology_graph_3, name_to_id, name_to_id_2, name_to_id_3, synonym_to_id, ont_number):\n",
        "    \"\"\"Builds a dict with candidates for all entity mentions in each document.\"\"\"\n",
        "\n",
        "    doc_count, nil_count, total_entities, total_unique_entities, no_solution, solution_is_first_count = int(), int(),int(), int(), int(), int()\n",
        "    documents_entity_list = dict() \n",
        "    doc_total = len(annotations.keys())\n",
        "    url_true = 0\n",
        "    for document in annotations.keys(): \n",
        "        doc_count += 1 \n",
        "        percent = round((doc_count/doc_total*100), 2)\n",
        "        print(\"Parsing document\", document, \"(\", doc_count, \"/\", doc_total, \" - \", str(percent), \"%)...\")\n",
        "        entity_dict = dict() \n",
        "        document_entities = list()        \n",
        "        url_control = ''\n",
        "        url_list = []\n",
        "        for annotation in annotations[document]:            \n",
        "            if annotation[0].isnumeric() == True and str(annotation).find('/') != -1:\n",
        "                if str(annotation) == url_control:\n",
        "                    url_true += 1\n",
        "                elif annotation in url_list:\n",
        "                    url_true += 1                    \n",
        "                continue\n",
        "            else:\n",
        "                normalized_text = annotation[0].lower().replace(\" \", \"_\")\n",
        "                total_entities += 1\n",
        "            \n",
        "                if normalized_text in document_entities: # Repeated instances of the same entity count as one instance\n",
        "                    continue\n",
        "                \n",
        "                else:    \n",
        "                    document_entities.append(normalized_text)\n",
        "                    total_unique_entities += 1\n",
        "                    # Get candidates for current entity\n",
        "                    entity_dict[normalized_text]= generate_candidates_for_entity(normalized_text, name_to_id, name_to_id_2, name_to_id_3,synonym_to_id, min_match_score, ontology_graph, ontology_graph_2, ontology_graph_3, ont_number)\n",
        "\n",
        "                if len(entity_dict[normalized_text]) == 0: # Do not consider this entity if no candidate was found\n",
        "                    del entity_dict[normalized_text]\n",
        "                    no_solution += 1\n",
        "                                     \n",
        "                else: # The entity has candidates, so it is added to entity_dict  \n",
        "                    entity_str = entity_string.format(annotation[0], normalized_text, \"MOR_NEO\", doc_count, document.strip(\".ann\"), \"NaN\")\n",
        "                    current_values = list()\n",
        "                    \n",
        "                    if model != \"string_matching\":\n",
        "                        current_values = entity_dict[normalized_text]\n",
        "                \n",
        "                    else:\n",
        "                        current_values = [entity_dict[normalized_text][0]] # Only the first solution will be considered (the most similar)\n",
        "                    \n",
        "                    current_values.insert(0, entity_str)\n",
        "                    entity_dict[normalized_text] = current_values\n",
        "                    url_control = entity_dict[normalized_text][1]['url']\n",
        "                    ind = 0\n",
        "                    url_list.clear()\n",
        "                    for url in entity_dict[normalized_text]:\n",
        "                        if ind != 0:\n",
        "                            url_list.append(url['url'])\n",
        "                        ind += 1\n",
        "                \n",
        "        documents_entity_list[document] = entity_dict\n",
        "    \n",
        "    # Output statistics\n",
        "    statistics = \"\\nNumber of documents: \" + str(len(documents_entity_list.keys())) \n",
        "    statistics += \"\\nTotal entities: \" + str(total_entities) + \"\\nNILs: \" + str(nil_count)\n",
        "    statistics += \"\\nValid entities: \" + str(total_entities-nil_count)\n",
        "    valid_entities_perc = ((total_entities-nil_count)/total_entities)*100\n",
        "    statistics += \"\\n % of valid entities: \" + str(valid_entities_perc)\n",
        "    statistics += \"\\n\\nTotal unique entities: \" + str(total_unique_entities)\n",
        "    statistics += \"\\n\\nTotal true url count: \" + str(url_true)\n",
        "    statistics += \"\\nTotal true url rate: %\" + str(url_true/total_unique_entities*100)\n",
        "    print(statistics)\n",
        "    return documents_entity_list\n",
        "    \n",
        "\n",
        "def pre_process():\n",
        "\n",
        "    start_time = time.time()\n",
        "    min_match_score = 0.1 # min lexical similarity between entity text and candidate text\n",
        "    ont_number = \"single_ont\" # ontologies to consider: single_ont (cieo3) or multi_ont (CIEO3, ICD10CM and DeCS)\n",
        "    model = \"ppr_ic\"  \n",
        "\n",
        "    if ont_number == \"multi_ont\": \n",
        "        ontology_graph_2, name_to_id_2 = load_spanish_icd10cm()\n",
        "        ontology_graph_3, name_to_id_3, synonym_to_id_3 = load_es_decs()\n",
        "    \n",
        "    elif ont_number == \"single_ont\":\n",
        "        ontology_graph_2, name_to_id_2,  ontology_graph_3, name_to_id_3 = list(), list(), list(), list()\n",
        "\n",
        "    ontology_graph, name_to_id, synonym_to_id  = load_cieo3() \n",
        "    annotations = parse_ner_output(True) \n",
        "    documents_entity_list = build_entity_candidate_dict(annotations, model, min_match_score, ontology_graph, ontology_graph_2,ontology_graph_3, name_to_id, name_to_id_2, name_to_id_3, synonym_to_id, ont_number)\n",
        "    \n",
        "    print(\"Parsing time (aprox.):\", int((time.time() - start_time)/60.0), \"minutes\\n----------------------------------\")\n",
        "    \n",
        "    # Get relations CIEO3<->ICD10CM and CIEO3<->DeCS from json files\n",
        "    extracted_relations_1, extracted_relations_2 = dict(), dict()\n",
        "\n",
        "    if model != \"string_matching\" and ont_number == \"multi-ont\":\n",
        "        \n",
        "        with open('./tmp/relations_cieo3_icd10cm.json', 'r') as json_file1:\n",
        "            extracted_relations_1 = json.load(json_file1)\n",
        "        json_file1.close()\n",
        "\n",
        "        with open('./tmp/relations_cieo3_esdecs.json', 'r') as json_file2:\n",
        "            extracted_relations_2 = json.load(json_file2)\n",
        "        json_file2.close()\n",
        "    \n",
        "    # Create a candidates file for each initial document\n",
        "    document_count, entities_writen = int(), int()\n",
        "        \n",
        "    for document in documents_entity_list:\n",
        "        document_count += 1\n",
        "        candidates_filename = \"./tmp/norm_candidates/single_ont/{}\".format(document.strip(\".ann\"))\n",
        "        print(\"Writing candidates:\\t\", document_count, \"/\", len(documents_entity_list.keys()))\n",
        "        entities_writen += write_candidates(documents_entity_list[document], candidates_filename, ontology_graph, ont_number, extracted_relations_1, extracted_relations_2)\n",
        "        \n",
        "    print(\"Entities writen in the candidates files:\", entities_writen)\n",
        "        \n",
        "    # Create file with the information content of each CIEO3 candidate appearing in candidates files (to use in ppr_ic model) \n",
        "    if model != \"string_matching\":\n",
        "        generate_ic_file(annotations, ontology_graph, ont_number)\n",
        "\n",
        "    print(\"Total time (aprox.):\", int((time.time() - start_time)/60.0), \"minutes\\n----------------------------------\")   \n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pre_process()\n",
        "         \n",
        "       \n"
      ],
      "metadata": {
        "id": "_Ac0qwndDSgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['happy']\n",
        "\n",
        "sentences2 = ['happier',\n",
        "              'happiest',\n",
        "              'very happy']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarits\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences2)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[0], sentences2[i], cosine_scores[0][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7cemtPp_4vf",
        "outputId": "72fbf9af-a0a2-4ab3-e67d-1b4bd5b06fd3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy \t\t happier \t\t Score: 0.8695\n",
            "happy \t\t happiest \t\t Score: 0.8140\n",
            "happy \t\t very happy \t\t Score: 0.9723\n"
          ]
        }
      ]
    }
  ]
}